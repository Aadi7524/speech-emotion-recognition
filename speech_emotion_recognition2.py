# -*- coding: utf-8 -*-
"""Speech_Emotion_Recognition2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GLn6BakVPNZfVJhZROD9CG-vwmNtQnoq
"""

from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os

zip_path = '/content/drive/My Drive/Voice_dataset.zip'  # path to your zip
extract_path = '/content/ravdess_dataset'  # where to extract it

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# Optional: Verify extraction
print("Extracted files:", os.listdir(extract_path))

import shutil

organized_path = "/content/ravdess_organized"

# Emotion label mapping
emotion_labels = {
    '01': 'neutral',
    '02': 'calm',
    '03': 'happy',
    '04': 'sad',
    '05': 'angry',
    '06': 'fearful',
    '07': 'disgust',
    '08': 'surprised'
}

# Create folders
for emotion in emotion_labels.values():
    os.makedirs(os.path.join(organized_path, emotion), exist_ok=True)

# Recursively go through all files and organize
for root, dirs, files in os.walk(extract_path):
    for file in files:
        if file.endswith('.wav'):
            emotion_code = file.split('-')[2]
            emotion = emotion_labels.get(emotion_code)
            if emotion:
                src = os.path.join(root, file)
                dest = os.path.join(organized_path, emotion, file)
                shutil.copy(src, dest)

print("✅ Dataset organized in:", organized_path)

import os
import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

def extract_mel_spectrogram(file_path, sr=22050, n_mels=128, fmax=8000):
    y, sr = librosa.load(file_path, sr=sr)
    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=fmax)
    log_mel = librosa.power_to_db(mel)
    return log_mel

# Install dependencies (if not done)
!pip install librosa --quiet

from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.callbacks import EarlyStopping
import json

# --- Audio augmentations ---
def add_noise(y):
    return y + 0.005 * np.random.randn(len(y))

def time_stretch(y):
    rate = np.random.uniform(0.95, 1.05)
    return librosa.effects.time_stretch(y, rate=rate)

def pitch_shift(y, sr):
    steps = np.random.randint(-1, 2)
    return librosa.effects.pitch_shift(y, sr=sr, n_steps=steps)

def augment_audio(y, sr):
    aug_type = np.random.choice(['pitch', 'stretch', 'noise', 'none'])
    if aug_type == 'pitch':
        return pitch_shift(y, sr)
    elif aug_type == 'stretch':
        return time_stretch(y)
    elif aug_type == 'noise':
        return add_noise(y)
    else:
        return y

# --- Mel spectrogram extraction ---
def extract_mel(file_path, sr=22050, n_mels=128, fmax=8000, augment=False):
    y, sr = librosa.load(file_path, sr=sr)
    if augment:
        y = augment_audio(y, sr)
    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=fmax)
    log_mel = librosa.power_to_db(mel)
    return librosa.util.fix_length(log_mel, size=130, axis=1)

# --- Load dataset ---
def load_data(data_dir):
    X, y = [], []
    emotions = sorted(os.listdir(data_dir))
    label_map = {emotion: i for i, emotion in enumerate(emotions)}

    for emotion in emotions:
        folder = os.path.join(data_dir, emotion)
        for file in os.listdir(folder):
            if file.endswith('.wav'):
                path = os.path.join(folder, file)

                # Original sample
                mel = extract_mel(path, augment=False)
                X.append(mel)
                y.append(label_map[emotion])

                # Augmented samples (2 per file)
                for _ in range(2):
                    mel_aug = extract_mel(path, augment=True)
                    X.append(mel_aug)
                    y.append(label_map[emotion])

    return np.array(X)[..., np.newaxis], to_categorical(y), label_map

data_dir = '/content/ravdess_organized'

def create_dataset(data_dir):
    X, y = [], []
    emotions = sorted(os.listdir(data_dir))  # Ensure consistent label order
    label_map = {emotion: i for i, emotion in enumerate(emotions)}

    for emotion in emotions:
        emotion_dir = os.path.join(data_dir, emotion)
        for file in os.listdir(emotion_dir):
            if file.endswith('.wav'):
                file_path = os.path.join(emotion_dir, file)
                mel_spec = extract_mel_spectrogram(file_path)

                # Resize (crop or pad) for consistent input shape
                mel_spec = librosa.util.fix_length(mel_spec, size=130, axis=1)

                X.append(mel_spec)
                y.append(label_map[emotion])

    X = np.array(X)
    y = np.array(y)
    return X, y, label_map

X, y, label_map = create_dataset(data_dir)
X = X[..., np.newaxis]  # Add channel dimension
X = X / np.max(X)       # Normalize
y = to_categorical(y)

print("Data shape:", X.shape)
print("Labels shape:", y.shape)
print("Emotion labels:", label_map)

import librosa
import numpy as np

def extract_multi_feature(file_path, sr=22050, n_mels=128, fmax=8000, augment=False):
    y, sr = librosa.load(file_path, sr=sr)

    if augment:
        y = augment_audio(y, sr)

    # Mel spectrogram
    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=fmax)
    mel_db = librosa.power_to_db(mel)
    mel_db = librosa.util.fix_length(mel_db, size=130, axis=1)

    # Pitch contour
    pitches, _ = librosa.piptrack(y=y, sr=sr)
    pitch_contour = np.mean(pitches, axis=0)
    pitch_contour = np.interp(np.linspace(0, len(pitch_contour), 130), np.arange(len(pitch_contour)), pitch_contour)
    pitch_contour = np.tile(pitch_contour, (128, 1))

    # Energy
    rms = librosa.feature.rms(y=y)[0]
    rms_interp = np.interp(np.linspace(0, len(rms), 130), np.arange(len(rms)), rms)
    rms_feature = np.tile(rms_interp, (128, 1))

    # Normalize
    mel_db = mel_db / np.max(np.abs(mel_db))
    pitch_contour = pitch_contour / np.max(pitch_contour + 1e-6)
    rms_feature = rms_feature / np.max(rms_feature + 1e-6)

    # Stack as 3 channels
    stacked = np.stack([mel_db, pitch_contour, rms_feature], axis=-1)
    return stacked

def load_data_multi(data_dir):
    X, y = [], []
    emotions = sorted(os.listdir(data_dir))
    label_map = {emotion: i for i, emotion in enumerate(emotions)}

    for emotion in emotions:
        folder = os.path.join(data_dir, emotion)
        for file in os.listdir(folder):
            if file.endswith('.wav'):
                path = os.path.join(folder, file)

                # Original
                X.append(extract_multi_feature(path, augment=False))
                y.append(label_map[emotion])

                # Targeted augmentation
                augment_count = 3 if emotion in ['fearful', 'surprised', 'neutral'] else 2
                for _ in range(augment_count):
                    X.append(extract_multi_feature(path, augment=True))
                    y.append(label_map[emotion])

    return np.array(X), to_categorical(y), label_map

from tensorflow.keras.layers import Layer
import tensorflow.keras.backend as K

class Attention(Layer):
    def __init__(self, **kwargs):
        super(Attention, self).__init__(**kwargs)

    def call(self, inputs):
        weights = K.softmax(K.sum(inputs, axis=-1, keepdims=True), axis=1)
        return K.sum(inputs * weights, axis=1)

from tensorflow.keras import models, layers

def build_model(input_shape, num_classes):
    model = models.Sequential([
        layers.Input(shape=input_shape),

        layers.Conv2D(32, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),

        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),

        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),

        layers.Reshape((14, -1)),

        layers.Bidirectional(layers.GRU(64, return_sequences=True)),
        Attention(),

        layers.Dense(128, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(num_classes, activation='softmax')
    ])
    return model

from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import train_test_split

X, y, label_map = load_data_multi("/content/ravdess_organized")
X = X / np.max(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Compute class weights
y_ints = np.argmax(y_train, axis=1)
weights = compute_class_weight('balanced', classes=np.unique(y_ints), y=y_ints)
class_weights = dict(enumerate(weights))

# Build and compile model
model = build_model(X.shape[1:], len(label_map))
model.compile(optimizer='adam',
              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),
              metrics=['accuracy'])

# Train
history = model.fit(X_train, y_train,
                    validation_data=(X_test, y_test),
                    epochs=50,
                    batch_size=32,
                    class_weight=class_weights,
                    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])

# --- Evaluate & Save ---
loss, acc = model.evaluate(X_test, y_test)
print(f"✅ Final Test Accuracy: {acc:.4f}")

model.save("human_emotion_model.h5")
with open("label_map.json", "w") as f:
    json.dump({k: int(v) for k, v in label_map.items()}, f)

from tensorflow.keras.layers import Layer
import tensorflow.keras.backend as K

class Attention(Layer):
    def __init__(self, **kwargs):
        super(Attention, self).__init__(**kwargs)

    def call(self, inputs):
        weights = K.softmax(K.sum(inputs, axis=-1, keepdims=True), axis=1)
        return K.sum(inputs * weights, axis=1)

from tensorflow.keras.models import load_model

model = load_model("human_emotion_model.h5", compile=False,
                   custom_objects={"Attention": Attention})

# Load label map
import json
with open("label_map.json", "r") as f:
    label_map = json.load(f)

inv_label_map = {v: k for k, v in label_map.items()}

import os

def predict_emotion(file_path):
    features = extract_multi_feature_for_prediction(file_path)
    pred = model.predict(features)[0]
    predicted_index = np.argmax(pred)
    predicted_label = inv_label_map[predicted_index]

    print(f"🎧 File: {os.path.basename(file_path)}")
    print(f"🧠 Predicted Emotion: {predicted_label}")
    print("📊 Probabilities:")
    for i, prob in enumerate(pred):
        print(f"  {inv_label_map[i]}: {prob:.2f}")

    return predicted_label

import librosa
import numpy as np

def extract_multi_feature_for_prediction(file_path, sr=22050, n_mels=128, fmax=8000):
    y, sr = librosa.load(file_path, sr=sr)

    # Mel spectrogram
    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=fmax)
    mel_db = librosa.power_to_db(mel)
    mel_db = librosa.util.fix_length(mel_db, size=130, axis=1)

    # Pitch contour
    pitches, _ = librosa.piptrack(y=y, sr=sr)
    pitch_contour = np.mean(pitches, axis=0)
    pitch_contour = np.interp(np.linspace(0, len(pitch_contour), 130), np.arange(len(pitch_contour)), pitch_contour)
    pitch_contour = np.tile(pitch_contour, (128, 1))

    # Energy (RMS)
    rms = librosa.feature.rms(y=y)[0]
    rms_interp = np.interp(np.linspace(0, len(rms), 130), np.arange(len(rms)), rms)
    rms_feature = np.tile(rms_interp, (128, 1))

    # Normalize and stack
    mel_db = mel_db / np.max(np.abs(mel_db))
    pitch_contour = pitch_contour / np.max(pitch_contour + 1e-6)
    rms_feature = rms_feature / np.max(rms_feature + 1e-6)

    stacked = np.stack([mel_db, pitch_contour, rms_feature], axis=-1)
    return stacked[np.newaxis, ...]

import librosa.display
import matplotlib.pyplot as plt
import numpy as np
import os

def predict_emotion_with_plot(file_path):
    # Load and extract features
    features = extract_multi_feature_for_prediction(file_path)
    pred = model.predict(features)[0]
    predicted_index = np.argmax(pred)
    predicted_label = inv_label_map[predicted_index]

    # Load raw audio and compute mel spectrogram for plotting
    y, sr = librosa.load(file_path, sr=22050)
    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)
    mel_db = librosa.power_to_db(mel, ref=np.max)

    # --- Display Mel Spectrogram ---
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(mel_db, sr=sr, x_axis='time', y_axis='mel', fmax=8000, cmap='magma')
    plt.title(f"Mel Spectrogram - Predicted Emotion: {predicted_label}")
    plt.colorbar(format='%+2.0f dB')
    plt.tight_layout()
    plt.show()

    # --- Print Prediction ---
    print(f"🎧 File: {os.path.basename(file_path)}")
    print(f"🧠 Predicted Emotion: {predicted_label}")
    print("📊 Probabilities:")
    for i, prob in enumerate(pred):
        print(f"  {inv_label_map[i]}: {prob:.2f}")

    return predicted_label

predict_emotion_with_plot("/content/ravdess_organized/fearful/03-01-06-01-02-02-01.wav")

test_files = [

    "/content/ravdess_organized/surprised/03-01-08-01-01-02-01.wav",
    "/content/ravdess_organized/neutral/03-01-01-01-01-01-01.wav",
    "/content/ravdess_organized/happy/03-01-03-01-02-01-01.wav",
    "/content/ravdess_organized/angry/03-01-05-01-01-01-01.wav",
    "/content/ravdess_organized/sad/03-01-04-01-01-01-01.wav",
]

for file in test_files:
    predict_emotion_with_plot(file)

# Save the trained model
model.save("human_emotion_model.h5")

# Save label map
with open("label_map.json", "w") as f:
    json.dump({k: int(v) for k, v in label_map.items()}, f)

!pip install -q gradio

import tensorflow as tf

# Load your old .h5 model
model = tf.keras.models.load_model("human_emotion_model.h5")

