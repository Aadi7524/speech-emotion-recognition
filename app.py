# -*- coding: utf-8 -*-
"""app.ipynb
Automatically generated by Colab.
Original file is located at
    https://colab.research.google.com/drive/1Zs0MP9ulmMTF42iyDqWoNUrm2mOissIT
"""

import gradio as gr
import librosa
import numpy as np
import tensorflow as tf
import json

# --- Attention Layer (custom) ---
class Attention(tf.keras.layers.Layer):
    def call(self, inputs):
        weights = tf.nn.softmax(tf.reduce_sum(inputs, axis=-1, keepdims=True), axis=1)
        return tf.reduce_sum(inputs * weights, axis=1)

# --- Load Model ---
model = tf.keras.models.load_model("human_emotion_model.h5", compile=False, custom_objects={"Attention": Attention})

# --- Load Label Map ---
with open("label_map.json", "r") as f:
    label_map = json.load(f)
inv_label_map = {v: k for k, v in label_map.items()}

# --- Feature Extraction Function ---
def extract_multi_feature(file_path, sr=22050, n_mels=128, fmax=8000):
    y, sr = librosa.load(file_path, sr=sr)

    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=fmax)
    mel_db = librosa.power_to_db(mel)
    mel_db = librosa.util.fix_length(mel_db, size=130, axis=1)

    pitches, _ = librosa.piptrack(y=y, sr=sr)
    pitch_contour = np.mean(pitches, axis=0)
    pitch_contour = np.interp(np.linspace(0, len(pitch_contour), 130), np.arange(len(pitch_contour)), pitch_contour)
    pitch_contour = np.tile(pitch_contour, (128, 1))

    rms = librosa.feature.rms(y=y)[0]
    rms_interp = np.interp(np.linspace(0, len(rms), 130), np.arange(len(rms)), rms)
    rms_feature = np.tile(rms_interp, (128, 1))

    mel_db = mel_db / np.max(np.abs(mel_db))
    pitch_contour = pitch_contour / np.max(pitch_contour + 1e-6)
    rms_feature = rms_feature / np.max(rms_feature + 1e-6)

    stacked = np.stack([mel_db, pitch_contour, rms_feature], axis=-1)
    return stacked[np.newaxis, ...]

# --- Prediction Function ---
def predict_emotion(audio_path):
    features = extract_multi_feature(audio_path)
    preds = model.predict(features)[0]
    top_emotions = {inv_label_map[i]: float(p) for i, p in enumerate(preds)}
    return top_emotions

# --- Gradio App ---
demo = gr.Interface(
    fn=predict_emotion,
    inputs=gr.Audio(type="filepath", label="Upload Speech (.wav)"),
    outputs=gr.Label(num_top_classes=3, label="Predicted Emotion"),
    title="üéôÔ∏è Speech Emotion Recognition",
    description="Upload or record an audio file to detect emotion using a CNN+BiGRU+Attention model trained on RAVDESS."
)

demo.launch()
