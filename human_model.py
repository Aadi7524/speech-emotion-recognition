# -*- coding: utf-8 -*-
"""human_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k8K_sA8E9ePKAhdHmCvi3YOcY94W9bB0
"""

from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os

zip_path = '/content/drive/My Drive/archive (3).zip'  # path to your zip
extract_path = '/content/ravdess_dataset'  # where to extract it

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# Optional: Verify extraction
print("Extracted files:", os.listdir(extract_path))

"""**another model**"""

import zipfile
import os

zip_path = '/content/drive/My Drive/archive (3).zip'
extract_path = "/content/ravdess_raw"

# Extract
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

!pip install tensorflow librosa matplotlib

import shutil

organized_path = "/content/ravdess_organized"

# Emotion label mapping
emotion_labels = {
    '01': 'neutral',
    '02': 'calm',
    '03': 'happy',
    '04': 'sad',
    '05': 'angry',
    '06': 'fearful',
    '07': 'disgust',
    '08': 'surprised'
}

# Create folders
for emotion in emotion_labels.values():
    os.makedirs(os.path.join(organized_path, emotion), exist_ok=True)

# Recursively go through all files and organize
for root, dirs, files in os.walk(extract_path):
    for file in files:
        if file.endswith('.wav'):
            emotion_code = file.split('-')[2]
            emotion = emotion_labels.get(emotion_code)
            if emotion:
                src = os.path.join(root, file)
                dest = os.path.join(organized_path, emotion, file)
                shutil.copy(src, dest)

print("âœ… Dataset organized in:", organized_path)

import os
import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

organized_path = '/content/ravdess_organized'

import os
import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import models, layers

def extract_mel_spectrogram(file_path, sr=22050, n_mels=128, fmax=8000):
    y, sr = librosa.load(file_path, sr=sr)
    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=fmax)
    log_mel = librosa.power_to_db(mel)
    return log_mel

data_dir = '/content/ravdess_organized'

def create_dataset(data_dir):
    X, y = [], []
    emotions = sorted(os.listdir(data_dir))  # Ensure consistent label order
    label_map = {emotion: i for i, emotion in enumerate(emotions)}

    for emotion in emotions:
        emotion_dir = os.path.join(data_dir, emotion)
        for file in os.listdir(emotion_dir):
            if file.endswith('.wav'):
                file_path = os.path.join(emotion_dir, file)
                mel_spec = extract_mel_spectrogram(file_path)

                # Resize (crop or pad) for consistent input shape
                mel_spec = librosa.util.fix_length(mel_spec, size=130, axis=1)

                X.append(mel_spec)
                y.append(label_map[emotion])

    X = np.array(X)
    y = np.array(y)
    return X, y, label_map

X, y, label_map = create_dataset(data_dir)
X = X[..., np.newaxis]  # Add channel dimension
X = X / np.max(X)       # Normalize
y = to_categorical(y)

print("Data shape:", X.shape)
print("Labels shape:", y.shape)
print("Emotion labels:", label_map)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = models.Sequential([
    layers.Input(shape=(128, 130, 1)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(len(label_map), activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy:.2f}")

audio_path = '/content/ravdess_organized/angry/03-01-05-01-01-01-01.wav'

def predict_emotion(audio_path):
    import librosa
    import librosa.display
    import numpy as np
    import matplotlib.pyplot as plt

    # Step 1: Extract mel spectrogram
    def extract_mel_spectrogram(file_path, sr=22050, n_mels=128, fmax=8000):
        y, sr = librosa.load(file_path, sr=sr)
        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=fmax)
        log_mel = librosa.power_to_db(mel)
        return log_mel

    mel = extract_mel_spectrogram(audio_path)

    # Step 2: Fix length to 130 frames (axis=1)
    mel = librosa.util.fix_length(mel, size=130, axis=1)

    # Step 3: Normalize and reshape
    mel = mel / np.max(mel)
    input_data = mel[np.newaxis, ..., np.newaxis]  # Shape: (1, 128, 130, 1)

    # Step 4: Predict
    pred = model.predict(input_data)
    label = list(label_map.keys())
    pred_label = label[np.argmax(pred)]

    # Step 5: Display
    print("ðŸŽ¤ Predicted Emotion:", pred_label)

    plt.figure(figsize=(10, 4))
    librosa.display.specshow(mel, sr=22050, x_axis='time', y_axis='mel', fmax=8000, cmap='magma')
    plt.colorbar(format='%+2.0f dB')
    plt.title(f'Predicted Emotion: {pred_label}')
    plt.tight_layout()
    plt.show()

    return pred_label

import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt

audio_path = '/content/ravdess_organized/happy/03-01-03-01-02-01-01.wav'
predict_emotion(audio_path)

audio_path = "/content/ravdess_organized/angry/03-01-05-01-01-01-01.wav"
predict_emotion(audio_path)

model.save("human_model.h5")

